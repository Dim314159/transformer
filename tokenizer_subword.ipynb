{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "853fbe0b-5366-45ed-a1ef-2788c5d9c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "#import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a073ee1-de66-47e2-be54-d475e32779ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caa2f70a-9251-4ead-a5b9-efd1929394f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = 'data/wiki/'\n",
    "#my_path = 'data/guten/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89500eb5-ccfd-4da5-ba81-3832389b2ad1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58a9d64-aa4b-4cd0-a100-df5ac99681fb",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d096a87a-0038-4f7b-a8fc-9da121164660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_line(line):\n",
    "    line = line.lower()\n",
    "    replace = [('\\\\', ' / '), ('[', ' ( '), ('{', ' ( '), (']', ' ) '), ('}', ' ) ')] # , ('/', ' / ')\n",
    "    for rep in replace:\n",
    "        if rep[0] in line:\n",
    "            line = line.replace(rep[0], rep[1])\n",
    "    line = re.sub('[^!-@_a-z ]', '', line)\n",
    "    line = re.findall(r\"[\\w']+|[^\\w\\s]\", line, re.UNICODE)\n",
    "    return line\n",
    "\n",
    "def update_pairs(pairs, pair, frequency, index = set(), remove = False):\n",
    "    pair_info = pairs.get(pair, [set(), 0])\n",
    "    if remove:\n",
    "        frequency = -frequency\n",
    "        pair_info[0].difference_update(index)\n",
    "    else:\n",
    "        pair_info[0] |= index\n",
    "    pair_info[1] = pair_info[1] + frequency\n",
    "    return pair_info\n",
    "\n",
    "def update_tokens(tokens, token, frequency = None, indices = set()):\n",
    "    if frequency is not None:\n",
    "        freq = tokens.get(token, 0)\n",
    "        result = freq + frequency\n",
    "    else:\n",
    "        inds = tokens.get(token, set())\n",
    "        result = indices.union(inds)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20f00d70-aa84-451e-9e07-f4ce88d7636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_ignore = [',', '.', '-', '(', ')', '\"', ':', '=', '&', ';', '!', '/', '$', '+', '%', '<', '>'\n",
    "                '_', '#', '?', '*', '@', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf5ebe8-a051-40d3-b5d7-9c09988c49f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4136782-22b2-4910-8572-54af7af12a07",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f58a96b2-cd4e-4e46-a433-dc21791c3de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = glob.glob(my_path + 'parts/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baed13f0-ae3c-4d19-a752-6bbdb76edd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_load = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58f27e7a-d6f0-4b48-b224-c1831a5f04fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_load:\n",
    "    with open(my_path + 'word_freq.json', 'r') as f:\n",
    "        word_freq = json.load(f)\n",
    "    with open(my_path + 'char_freq.json', 'r') as f:\n",
    "        char_freq = json.load(f)\n",
    "else:\n",
    "    word_freq = Counter()\n",
    "    char_freq = Counter()\n",
    "    for i, file_name in enumerate(texts):\n",
    "        new_text = []\n",
    "        with open(file_name, 'r') as f:\n",
    "            for line in f:\n",
    "                words = clean_line(line)\n",
    "                word_freq.update(words)\n",
    "                new_text.extend(words)\n",
    "                for word in words:\n",
    "                    if word in chars_ignore:\n",
    "                        chars = list(word)\n",
    "                    else:\n",
    "                        chars = list(word) + ['^']\n",
    "                    char_freq.update(chars)\n",
    "        fn = str(i) + '.txt'\n",
    "        with open(my_path + 'clean_parts/part_' + fn, 'w') as f:\n",
    "            for word in new_text:\n",
    "                f.write('%s\\n' % word)\n",
    "    with open(my_path + 'word_freq.json', 'w') as f:\n",
    "        json.dump(word_freq, f)\n",
    "    with open(my_path + 'char_freq.json', 'w') as f:\n",
    "        json.dump(char_freq, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d597050-e271-404a-9fc3-bfca460ab665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed915d84-bd73-4717-8719-b72f41655825",
   "metadata": {},
   "outputs": [],
   "source": [
    "created_tokens = {}\n",
    "with open(my_path + 'char_freq.json', 'r') as f:\n",
    "    token_freq = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e4245-6f95-4f39-80ea-9374802619ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "505462c1-2c7e-4253-b28f-a6106eba8b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_load = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d341c97-8216-4749-9df3-ad4e30e6b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_load:\n",
    "    with open(my_path + 'words_to_index.json', 'r') as f:\n",
    "        words_to_index = json.load(f)\n",
    "    with open(my_path + 'index_to_words.json', 'r') as f:\n",
    "        index_to_words = json.load(f)\n",
    "    with open(my_path + 'words_tokenized.json', 'r') as f:\n",
    "        words_tokenized = json.load(f)\n",
    "else:\n",
    "    words_to_index = {}\n",
    "    index_to_words = []\n",
    "    words_tokenized = []\n",
    "    for i, word in enumerate(word_freq):\n",
    "        words_to_index[word] = i\n",
    "        index_to_words.append(word)\n",
    "        if word in chars_ignore:\n",
    "            token_word = list(word)\n",
    "        else:\n",
    "            token_word = list(word) + ['^']\n",
    "        words_tokenized.append(token_word)\n",
    "    with open(my_path + 'words_to_index.json', 'w') as f:\n",
    "        json.dump(words_to_index, f)\n",
    "    with open(my_path + 'index_to_words.json', 'w') as f:\n",
    "        json.dump(index_to_words, f)\n",
    "    with open(my_path + 'words_tokenized.json', 'w') as f:\n",
    "        json.dump(words_tokenized, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06725843-483b-41ab-beae-6dc229788555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "476891ac-ea39-4c37-b58a-75fb7e08be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_reset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79a2e0ff-eb93-4e5c-ac13-8238b838801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_reset:\n",
    "    created_tokens = {}\n",
    "    with open(my_path + 'char_freq.json', 'r') as f:\n",
    "        token_freq = json.load(f)\n",
    "    with open(my_path + 'words_tokenized.json', 'r') as f:\n",
    "        words_tokenized = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532104ce-1e1e-4e43-ba60-114123ad8611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "940a03d7-91fb-456c-bfce-63d59661615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_load = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "214dc3f1-ec3a-47c0-b33c-c54157ce63f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_load:\n",
    "    with open(my_path + 'pairs.pkl', 'rb') as f:\n",
    "        pairs = pickle.load(f)\n",
    "else:\n",
    "    pairs = {}\n",
    "    for index, tokens in enumerate(words_tokenized):\n",
    "        tokens_end = len(tokens) - 1\n",
    "        frequency = word_freq[index_to_words[index]]\n",
    "        counter = 0\n",
    "        for i in range(tokens_end):\n",
    "            first = tokens[i]\n",
    "            if first in chars_ignore:\n",
    "                counter = 0\n",
    "                continue\n",
    "            second = tokens[i + 1]\n",
    "            if second in chars_ignore:\n",
    "                counter = 0\n",
    "                continue\n",
    "            if first == second:\n",
    "                counter += 1\n",
    "            else:\n",
    "                counter = 0\n",
    "            if counter > 1:\n",
    "                counter = 0\n",
    "                continue\n",
    "            pair = first, second\n",
    "            pairs[pair] = update_pairs(pairs, pair, frequency, {index})\n",
    "\n",
    "    with open(my_path + 'pairs.pkl', 'wb') as f:\n",
    "        pickle.dump(pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ee57d-ecf5-4448-b172-2781075e8058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03a852f7-c017-4a84-aced-e1f5de9ec9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1395454\n",
      "43\n",
      "43\n",
      "1395454\n",
      "1395454\n",
      "1395454\n",
      "757\n"
     ]
    }
   ],
   "source": [
    "print(len(word_freq)) # dict word : amount used\n",
    "print(len(char_freq)) # dict char : amount used (initial)\n",
    "print(len(token_freq)) # dict char : amount used\n",
    "print(len(words_to_index)) # dict word : index of the word\n",
    "print(len(index_to_words)) # list of words\n",
    "print(len(words_tokenized)) # list of tokenized words\n",
    "print(len(pairs)) # dict (tuple of pair of tokens) : ({indices of words contained this pair}, amount used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7787e10-d873-4a65-98b3-90d3b9d6f4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of words 295402752\n"
     ]
    }
   ],
   "source": [
    "words_total = 0\n",
    "for v in word_freq.values():\n",
    "    words_total += v\n",
    "print('amount of words', words_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f181ddb-1df3-4eb2-acf8-f32ef853663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_continue = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f007e15-5cc4-4c2b-84f2-0f3f1d3fbb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_continue:\n",
    "    with open(my_path + 'token_freq_1.json', 'r') as f:\n",
    "        token_freq = json.load(f)\n",
    "    with open(my_path + 'created_tokens_1.pkl', 'rb') as f:\n",
    "        created_tokens = pickle.load(f)\n",
    "    with open(my_path + 'words_tokenized_1.json', 'r') as f:\n",
    "        words_tokenized = json.load(f)\n",
    "    with open(my_path + 'pairs_1.pkl', 'rb') as f:\n",
    "        pairs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee68c3c-297d-48c6-a2c6-81b8d9a5badc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a2e8a6d-be03-4e63-87dc-990d22eb5824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "757"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71abc9fe-d081-4150-857e-9867c17ecc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_plot = []\n",
    "for val in pairs.values():\n",
    "    pairs_plot.append(val[1])\n",
    "pairs_plot.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7944388-f5c1-4f40-a2d4-54da9e036c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pairs_plot[:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532be305-ca14-424d-91ad-7de7f50de93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83022a96-1fde-490b-9638-a0843551506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_full = total_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f464b3d7-742a-46e8-b445-4a0750996a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_save_ep = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b948922-5ac8-4e26-b742-e876161d8ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048576\n",
      "1024\n",
      "32768\n"
     ]
    }
   ],
   "source": [
    "pair_min_amount = 2 ** 20 # based on pair frequency\n",
    "print(pair_min_amount)\n",
    "abs_pair_min = 2 ** 10\n",
    "print(abs_pair_min)\n",
    "vocab_size = 2 ** 15\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e4bbd-c55c-4a37-86f0-8ab82d857ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5cb1837-7a94-4fc9-bd42-3d0603f9dadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n"
     ]
    }
   ],
   "source": [
    "epochs = 2 ** 13\n",
    "print(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e13dd287-691e-40eb-8335-6520c3b0c42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========                 ] 32%, ep: 2640 , 00:00:16:21, tokens: 14617, min_freq: 1024, voc_f/tot_ep: 0/19024\n",
      "no_more_frequent_pairs\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "start_time = dt.now()\n",
    "for ep in range(1, epochs + 1):\n",
    "    # find frequent pair\n",
    "    flag_break = False\n",
    "    flag_seek = True\n",
    "    max_value = -1\n",
    "    while flag_seek:\n",
    "        for pair, value in pairs.items():\n",
    "            if value[1] > pair_min_amount:\n",
    "                cur_value = value[1] / (token_freq[pair[0]] * token_freq[pair[1]])\n",
    "                if cur_value > max_value:\n",
    "                    max_pair = pair\n",
    "                    max_value = cur_value\n",
    "                    flag_seek = False\n",
    "\n",
    "        if flag_seek:\n",
    "            pair_min_amount = pair_min_amount // 2\n",
    "            if pair_min_amount < abs_pair_min:\n",
    "                print('\\nno_more_frequent_pairs')\n",
    "                flag_break = True\n",
    "                flag_seek = False\n",
    "    if flag_break:\n",
    "        break\n",
    "            \n",
    "    total_epochs += 1\n",
    "\n",
    "    max_pair_indices = pairs[max_pair][0].copy()\n",
    "    max_pair_count = pairs[max_pair][1]\n",
    "\n",
    "    #print(max_pair, max_pair_count)\n",
    "\n",
    "    new_token = ''.join(max_pair)\n",
    "    token_freq[new_token] = update_tokens(token_freq, new_token, frequency = max_pair_count)  # UPDATING\n",
    "    created_tokens[new_token] = update_tokens(created_tokens, new_token, indices = max_pair_indices)  # creating\n",
    "    for tok in max_pair:\n",
    "        token_freq[tok] -= max_pair_count   # UPDATING token frequency\n",
    "\n",
    "    # remove all pairs from words with max pair\n",
    "    for index in max_pair_indices:\n",
    "        tokens = words_tokenized[index]\n",
    "        tokens_end = len(tokens) - 1\n",
    "        frequency = word_freq[index_to_words[index]]\n",
    "        locations = []\n",
    "        counter = 0\n",
    "        for i in range(tokens_end):\n",
    "            first = tokens[i]\n",
    "            if first in chars_ignore:\n",
    "                counter = 0\n",
    "                continue\n",
    "            second = tokens[i + 1]\n",
    "            if second in chars_ignore:\n",
    "                counter = 0\n",
    "                continue\n",
    "            if first == second:\n",
    "                counter += 1\n",
    "            else:\n",
    "                counter = 0\n",
    "            if counter > 1:\n",
    "                counter = 0\n",
    "                continue\n",
    "            pair = first, second\n",
    "            pairs[pair] = update_pairs(pairs, pair, frequency, {index}, remove = True) # Updating pairs\n",
    "            if pairs[pair][1] < 1:\n",
    "                del pairs[pair]        # Updating pairs\n",
    "            if pair == max_pair:\n",
    "                locations.append(i)\n",
    "\n",
    "        # update tokenized word with new created token\n",
    "        new_tokens = []\n",
    "        point = 0\n",
    "        for i in locations:\n",
    "            new_tokens = new_tokens + tokens[point : i]\n",
    "            point = i + 2\n",
    "            new_tokens.append(new_token)\n",
    "        new_tokens = new_tokens + tokens[point:]\n",
    "        words_tokenized[index] = new_tokens      # Updating tokenized words\n",
    "\n",
    "        tokens_end = len(new_tokens) - 1\n",
    "        counter = 0\n",
    "        for i in range(tokens_end):\n",
    "            first = new_tokens[i]\n",
    "            if first in chars_ignore:\n",
    "                counter = 0\n",
    "                continue\n",
    "            second = new_tokens[i + 1]\n",
    "            if second in chars_ignore:\n",
    "                counter = 0\n",
    "                continue\n",
    "            if first == second:\n",
    "                counter += 1\n",
    "            else:\n",
    "                counter = 0\n",
    "            if counter > 1:\n",
    "                counter = 0\n",
    "                continue\n",
    "            pair = first, second\n",
    "            pairs[pair] = update_pairs(pairs, pair, frequency, {index})      # Updating pairs\n",
    "\n",
    "    # remove tokens with low frequency\n",
    "    for tok in max_pair:\n",
    "        if tok in created_tokens and token_freq[tok] <= pair_min_amount:\n",
    "            indices = created_tokens[tok].copy()\n",
    "            del created_tokens[tok]    # deleting created token from created tokens\n",
    "            del token_freq[tok]        # deleting token from tokens\n",
    "            for index in indices:\n",
    "                tokens = words_tokenized[index]\n",
    "                tokens_end = len(tokens) - 1\n",
    "                frequency = word_freq[index_to_words[index]]\n",
    "\n",
    "                counter = 0\n",
    "                for i in range(tokens_end):\n",
    "                    first = tokens[i]\n",
    "                    if first in chars_ignore:\n",
    "                        counter = 0\n",
    "                        continue\n",
    "                    second = tokens[i + 1]\n",
    "                    if second in chars_ignore:\n",
    "                        counter = 0\n",
    "                        continue\n",
    "                    if first == second:\n",
    "                        counter += 1\n",
    "                    else:\n",
    "                        counter = 0\n",
    "                    if counter > 1:\n",
    "                        counter = 0\n",
    "                        continue\n",
    "                    pair = first, second\n",
    "                    pairs[pair] = update_pairs(pairs, pair, frequency, {index}, remove = True) # Updating pairs\n",
    "                    if pairs[pair][1] < 1:\n",
    "                        del pairs[pair]        # Updating pairs\n",
    "\n",
    "                locations = []\n",
    "                for l, t in enumerate(tokens):\n",
    "                    if t == tok:\n",
    "                        locations.append(l)\n",
    "\n",
    "                tok_amount = len(locations)\n",
    "                original_chars = list(tok)\n",
    "                for char in original_chars:\n",
    "                    token_freq[char] += frequency * tok_amount # UPDATING\n",
    "\n",
    "                new_tokens = []\n",
    "                point = 0\n",
    "                for i in locations:\n",
    "                    new_tokens = new_tokens + tokens[point : i] + original_chars\n",
    "                    point = i + 1\n",
    "                new_tokens = new_tokens + tokens[point:]\n",
    "                words_tokenized[index] = new_tokens      # Updating tokenized words\n",
    "\n",
    "                tokens_end = len(new_tokens) - 1\n",
    "                counter = 0\n",
    "                for i in range(tokens_end):\n",
    "                    first = new_tokens[i]\n",
    "                    if first in chars_ignore:\n",
    "                        counter = 0\n",
    "                        continue\n",
    "                    second = new_tokens[i + 1]\n",
    "                    if second in chars_ignore:\n",
    "                        counter = 0\n",
    "                        continue\n",
    "                    if first == second:\n",
    "                        counter += 1\n",
    "                    else:\n",
    "                        counter = 0\n",
    "                    if counter > 1:\n",
    "                        counter = 0\n",
    "                        continue\n",
    "                    pair = first, second\n",
    "                    pairs[pair] = update_pairs(pairs, pair, frequency, {index})      # Updating pairs\n",
    "\n",
    "\n",
    "    if len(token_freq) == vocab_size - 1:\n",
    "        voc_save_ep.append(total_epochs)\n",
    "        vocab_full = total_epochs\n",
    "        #print('vocab is full')\n",
    "        with open(my_path + 'token_freq_0.json', 'w') as f:\n",
    "            json.dump(token_freq, f)\n",
    "        with open(my_path + 'created_tokens_0.pkl', 'wb') as f:\n",
    "            pickle.dump(created_tokens, f)\n",
    "        with open(my_path + 'words_tokenized_0.json', 'w') as f:\n",
    "            json.dump(words_tokenized, f)\n",
    "        with open(my_path + 'pairs_0.pkl', 'wb') as f:\n",
    "            pickle.dump(pairs, f)\n",
    "        #break\n",
    "\n",
    "    if total_epochs % 100 == 0:\n",
    "        with open(my_path + 'token_freq_1.json', 'w') as f:\n",
    "            json.dump(token_freq, f)\n",
    "        with open(my_path + 'created_tokens_1.pkl', 'wb') as f:\n",
    "            pickle.dump(created_tokens, f)\n",
    "        with open(my_path + 'words_tokenized_1.json', 'w') as f:\n",
    "            json.dump(words_tokenized, f)\n",
    "        with open(my_path + 'pairs_1.pkl', 'wb') as f:\n",
    "            pickle.dump(pairs, f)\n",
    "    if ep % 10 == 0:\n",
    "        elapsed = dt.now() - start_time\n",
    "        secs = elapsed.seconds\n",
    "        ratio = ep / epochs\n",
    "        printout = '\\r[%-25s] %d%%, ep: %d , %02d:%02d:%02d:%02d, tokens: %d, min_freq: %d, voc_f/tot_ep: %d/%d'\n",
    "        sys.stdout.write(printout % ('='*round(25 * ratio), round(100 * ratio), ep, elapsed.days, secs // 3600, \n",
    "                            secs // 60 % 60, secs %60, len(token_freq), pair_min_amount, vocab_full, total_epochs))\n",
    "        sys.stdout.flush()\n",
    "sys.stdout.write('\\n')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "69491de5-3dee-441d-b0f9-aa6485c4eef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_save_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81913f6e-08dd-4139-8fa4-1507253b17ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8bc8bb0e-fcc8-407e-84e1-3e7df77aee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_save:\n",
    "    with open(my_path + 'token_freq_0.json', 'w') as f:\n",
    "        json.dump(token_freq, f)\n",
    "    with open(my_path + 'created_tokens_0.pkl', 'wb') as f:\n",
    "        pickle.dump(created_tokens, f)\n",
    "    with open(my_path + 'words_tokenized_0.json', 'w') as f:\n",
    "        json.dump(words_tokenized, f)\n",
    "    with open(my_path + 'pairs_0.pkl', 'wb') as f:\n",
    "        pickle.dump(pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64930e-92c7-4d35-9f1e-fedc450129b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4526c22-2608-4c90-ae37-eb5fc0fd9af0",
   "metadata": {},
   "source": [
    "# Train tokenizer to tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51bede14-caa6-421a-af1f-5d802f6f214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_dataset(Dataset):\n",
    "    def __init__(self, pairs_encoded):\n",
    "        self.pairs = pairs_encoded\n",
    "        self.dataset_size = len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        question = torch.LongTensor(self.pairs[i][0])\n",
    "        reply = torch.LongTensor(self.pairs[i][1])\n",
    "        return question, reply\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e8d7e62-7167-4d9b-8026-eded48691466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, model_optimizer, train_hp, save = False):\n",
    "    epochs, batch_size, my_path, model_name = train_hp\n",
    "    loss_train = []\n",
    "    start_time = dt.now()\n",
    "    model.train()\n",
    "    \n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.epochs += 1\n",
    "        loss_tempo = []\n",
    "        j = 0\n",
    "        ep_length = len(train_loader)\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            y_input = y[..., :-1]\n",
    "            y_target = y[..., 1:]\n",
    "            \n",
    "            q_mask, r_mask, rt_mask = create_masks(x, y_input, y_target)\n",
    "            \n",
    "            out = model(x, q_mask, y_input, r_mask)\n",
    "            loss = criterion(out, y_target, rt_mask)\n",
    "            \n",
    "            model_optimizer.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            model_optimizer.step()\n",
    "            \n",
    "            loss_t = loss.item()\n",
    "            loss_tempo.append(loss_t)\n",
    "            j += 1\n",
    "            if j % 100 == 0:\n",
    "                elapsed = dt.now() - start_time\n",
    "                secs = elapsed.seconds\n",
    "                ratio = (j + 1) / ep_length\n",
    "                printout = '\\r[%-16s] %d%%, batch: %d, %02d:%02d:%02d:%02d, epoch: %d/%d, total: %d, loss: %f'\n",
    "                sys.stdout.write(printout % ('='*round(16 * ratio), round(100 * ratio), j, elapsed.days, \n",
    "                                        secs // 3600, secs // 60 % 60, secs % 60, ep, epochs, model.epochs, loss_t))\n",
    "                sys.stdout.flush()\n",
    "            \n",
    "        ep_loss = np.mean(np.asarray(loss_tempo), axis=0)\n",
    "        loss_train.extend(loss_tempo)\n",
    "        model.losses.append(ep_loss)\n",
    "        \n",
    "        if save:\n",
    "            state = {'model': model, 'model_optimizer': model_optimizer, 'criterion': criterion, \n",
    "                     'model_hp' : model_hp}\n",
    "            torch.save(state, my_path + model_name)\n",
    "    \n",
    "    sys.stdout.write('\\n')\n",
    "    return loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b74fdadd-e716-4202-91aa-22fc8a867369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss):\n",
    "    plt.plot(loss)\n",
    "    #plt.xlabel('epochs', fontsize=20)\n",
    "    plt.ylabel('loss', fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "379d53e4-598c-45da-bc04-42cef6e120a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_word(model, question, word_length, token_map, rev_token_map, parallel):\n",
    "    \"\"\"\n",
    "    Performs Greedy Decoding with a batch size of 1\n",
    "    \"\"\"    \n",
    "    \n",
    "    question = list(question) + ['^'] \n",
    "    question = [token_map[t] for t in question]\n",
    "    question = torch.LongTensor(question).to(device).unsqueeze(0)\n",
    "    question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)\n",
    "    start_token = token_map['^']\n",
    "    reply = [[start_token]]\n",
    "    reply = torch.LongTensor(reply).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step in range(word_length - 1):\n",
    "            dims = reply.size(-1)\n",
    "            reply_mask = torch.triu(torch.ones(dims, dims)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "            reply_mask = reply_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
    "            if parallel:\n",
    "                decoded = model.encode_decode(question, question_mask, reply, reply_mask)\n",
    "            else:\n",
    "                encoded = model.encode(question, question_mask)\n",
    "                decoded = model.decode(encoded, question_mask, reply, reply_mask)\n",
    "            predictions = model.out(decoded[:, -1])\n",
    "            next_word = torch.argmax(predictions, dim = 1)\n",
    "            next_word = next_word.item()\n",
    "            reply = torch.cat([reply, torch.LongTensor([[next_word]]).to(device)], dim = 1)\n",
    "            if rev_token_map[next_word][-1] == '^':\n",
    "                break\n",
    "\n",
    "        # Construct Sentence\n",
    "        if reply.dim() == 2:\n",
    "            reply = reply.squeeze(0)\n",
    "            reply = reply.tolist()\n",
    "\n",
    "        word = [rev_token_map[k] for k in reply[1:]]\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118f9270-cfa5-4cb0-9a4e-c0899c0dcb46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "105257e3-aa0a-426b-b9f7-9760ef0a0358",
   "metadata": {},
   "source": [
    "# prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c12c05f4-319d-4dd6-b06c-b28340f6605a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/wiki/\n"
     ]
    }
   ],
   "source": [
    "print(my_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "065b1b76-30ab-4aa9-a16b-768b8e190169",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_load = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d059735-e5a8-45b4-9b25-c6892c80cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_load:\n",
    "    with open(my_path + 'token_map.json', 'r') as f:\n",
    "        token_map = json.load(f)\n",
    "else:\n",
    "    with open(my_path + 'token_freq_0.json', 'r') as f:\n",
    "        token_freq = json.load(f)\n",
    "    token_map = {k : v + 1 for v, k in enumerate(token_freq)}\n",
    "    #token_map['<unk>'] = len(token_map) + 1\n",
    "    #token_map['<start>'] = len(token_map) + 1\n",
    "    token_map['<pad>'] = 0\n",
    "    \n",
    "with open(my_path + 'words_to_index.json', 'r') as f:\n",
    "    words_to_index = json.load(f)\n",
    "with open(my_path + 'words_tokenized_0.json', 'r') as f:\n",
    "    words_tokenized = json.load(f)\n",
    "    \n",
    "with open(my_path + 'char_freq.json', 'r') as f:\n",
    "    char_freq = json.load(f)\n",
    "char_map = {k : v + 1 for v, k in enumerate(char_freq)}\n",
    "#token_map['<unk>'] = len(token_map) + 1\n",
    "#token_map['<start>'] = len(token_map) + 1\n",
    "char_map['<pad>'] = 0\n",
    "    \n",
    "rev_token_map = {v: k for k, v in token_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b422fd0-ddef-4152-8cda-8d353bacf23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abb5ba8e-ea06-4b0d-bdce-d9d13ef5976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_save:\n",
    "    with open(my_path + 'token_map.json', 'w') as f:\n",
    "        json.dump(token_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f62cf7f-a843-407d-9834-081524ac9629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "vocab_size_in = len(char_map)\n",
    "print(vocab_size_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8accac90-2048-4152-9f84-b959afb7259d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23756\n"
     ]
    }
   ],
   "source": [
    "vocab_size_out = len(token_map)\n",
    "print(vocab_size_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19ae7b04-7ca3-4a64-b1cb-fb23688016f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_length = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15303c09-6f8d-4c32-9820-ea7009942777",
   "metadata": {},
   "source": [
    "## create data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9c0cab6-8184-4d35-be8e-5c94b0450394",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens_pairs = []\n",
    "for w, i in words_to_index.items():\n",
    "    if len(w) < word_length:\n",
    "        pads = [char_map['<pad>']] * (word_length - len(w) - 1)\n",
    "        word = list(w) + ['^'] \n",
    "        word = [char_map[t] for t in word] + pads\n",
    "        toks = words_tokenized[i]\n",
    "        pads = [token_map['<pad>']] * (word_length - len(toks))\n",
    "        word_toked = [token_map['^']] + [token_map[t] for t in toks] + pads\n",
    "        word_tokens_pairs.append((word, word_toked))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e48e11b-1473-4ad0-8153-43b099594d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "705557fd-85fd-4185-bf85-8f19067ec9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(My_dataset(word_tokens_pairs),\n",
    "                                           batch_size = batch_size, \n",
    "                                           shuffle=True, \n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c13d1-890b-40c1-8819-aaead6b68cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4fffdc6-6584-49f8-9f6e-02bbab47d148",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b556161-32bc-4f12-a64d-5f75d12ce1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Transformer_class.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60cebc5e-8a34-41b9-93b0-47f7bebbdadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.set_device(1)\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c38d13ab-b453-42f7-b4c5-65b066abe3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b77fa55-f998-433a-8fd6-ee81aacaa6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_order(d_emb, repeat, deep):\n",
    "    order = []\n",
    "    hidden = [d_emb * 2, d_emb, d_emb, d_emb // 2]\n",
    "    for dh in hidden:\n",
    "        h = dh // 16\n",
    "        for _ in range(deep):\n",
    "            for _ in range(repeat):\n",
    "                order.append((dh, h))\n",
    "            h = max(1, h // 2)\n",
    "    return order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fe560ec-b1c6-45c5-bb8d-61e1a5cc7fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_emb = 128\n",
    "hidden_mult = 4\n",
    "#order = create_order(d_emb, 1, 4)\n",
    "# order = [(256, 16), (256, 16), (256, 8), (256, 8), \n",
    "#          (128, 8), (128, 8), (128, 4), (128, 4), \n",
    "#          (128, 4), (128, 4), (128, 2), (128, 2), \n",
    "#          (64, 2), (64, 2), (64, 1), (64, 1)]\n",
    "order = [(256, 16),   \n",
    "         (128, 8),   \n",
    "         (128, 4),   \n",
    "         (64, 2), (64, 1)]\n",
    "dropouts = [0, 0, 0]\n",
    "parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebc4941f-d16c-449c-adfa-6f708f41fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_load = False\n",
    "model_name = 'tokenizer.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2b2e552-6b51-4e67-a7c2-2543d12c55a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_load:\n",
    "    state = torch.load(my_path + model_name)\n",
    "    model_hp = state['model_hp']\n",
    "    model = state['model']\n",
    "    model_optimizer = state['model_optimizer']\n",
    "    criterion = state['criterion']\n",
    "    vocab_size_in, vocab_size_out, d_emb, hidden_mult,  word_length, order, dropouts, parallels = model_hp\n",
    "else:\n",
    "    model_hp = vocab_size_in, vocab_size_out, d_emb, hidden_mult,  word_length, order, dropouts, parallel\n",
    "    model = Transformer(model_hp).to(device)\n",
    "    #model = nn.DataParallel(model, device_ids = [0, 1, 2, 3])\n",
    "    #model.to(f'cuda:{model.device_ids[0]}')\n",
    "    l2 = 0.0\n",
    "    learning_rate = 0\n",
    "    warmup_steps = 4000\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=l2, amsgrad=False)\n",
    "    model_optimizer = AdamWarmup(d_emb, warmup_steps, optimizer)\n",
    "    criterion = LossWithLS(vocab_size_out, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f9f4e43-3b64-4baa-b59a-16a052ec5424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size_in 59\n",
      "vocab_size_out 23756\n",
      "d_emb 128\n",
      "hidden_mult 4\n",
      "word_length 32\n",
      "order [(256, 16), (128, 8), (128, 4), (64, 2), (64, 1)]\n",
      "dropouts [0, 0, 0]\n",
      "parallel True\n",
      "num of parameters: 11503308\n"
     ]
    }
   ],
   "source": [
    "print('vocab_size_in', vocab_size_in)\n",
    "print('vocab_size_out', vocab_size_out)\n",
    "print('d_emb', d_emb)\n",
    "print('hidden_mult', hidden_mult)\n",
    "print('word_length', word_length)\n",
    "print('order', order)\n",
    "print('dropouts', dropouts)\n",
    "print('parallel', parallel)\n",
    "print('num of parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998cdb0a-4adc-4845-8fc3-88af7d91c5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41964f37-70dc-4dda-8dbb-4ee6418dedc0",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e006854f-f9de-463f-a250-e94dde29d9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa93810e-2123-4e19-ab3a-146a1ecd539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hp = epochs, batch_size, my_path, model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c50cc1-29ac-4a6d-9f0e-666dbdb48b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========        ] 48%, batch: 60900, 03:01:34:39, epoch: 15/20, total: 15, loss: 0.07921162"
     ]
    }
   ],
   "source": [
    "loss_train = train(model, train_loader, criterion, model_optimizer, train_hp, save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "102fe431-a699-4da9-86a9-ffc89c27601d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(loss_train)\n",
    "plot_loss(model.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9befd5-7139-4eee-a93b-f3a7c0b37a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9e862660-de23-4b74-95b8-bb4e450b1fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question:  question\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question^']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question:  tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'ke', 'ni', 'zer^']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question:  abracadabra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'bra', 'ca', 'da', 'bra^']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question:  quit\n"
     ]
    }
   ],
   "source": [
    "while(1):\n",
    "    question = input(\"Question: \") \n",
    "    if question == 'quit':\n",
    "        break\n",
    "    question = clean_line(question)[0]\n",
    "    sentence = tokenize_word(model, question, word_length, token_map, rev_token_map, parallel)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a92d6c-c693-4ced-8ab9-20cd646a6cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bd89ce19-22c8-4c9c-a664-86b1cb849fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9935c31e-76bb-496f-b171-7d4bcc1c283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_save:\n",
    "    state = {'model': model, 'model_optimizer': model_optimizer, 'criterion': criterion, 'model_hp' : model_hp}\n",
    "    torch.save(state, my_path + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c902fd2f-2b63-45e0-bec4-b42b2f8c7e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d34bdf-b59b-4829-8f5d-afa48dac5644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d84c1c72-206f-4ea6-bb99-acd0b55aa0cf",
   "metadata": {},
   "source": [
    "# Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c4d61408-7388-4311-98ee-bc9fc4fde1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explo\n",
      "express\n",
      "juli\n",
      "jun\n",
      "jud\n",
      "ver\n",
      "ave\n",
      "have\n",
      "hou\n",
      "house\n",
      "doubt\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for tok in token_freq:\n",
    "    i += 1\n",
    "    if i > 100:\n",
    "        print(tok)\n",
    "    if i > 110:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "005ed79a-547c-4307-9933-70b0a9b60f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the^']\n",
      "['big^']\n",
      "['pine^']\n",
      "['band^']\n",
      "['of^']\n",
      "['ow', 'ens^']\n",
      "['valley^']\n",
      "['pa', 'i', 'ute^']\n",
      "['indians^']\n",
      "['have^']\n"
     ]
    }
   ],
   "source": [
    "for toks in words_tokenized[:10]:\n",
    "    print(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f631c4-7ce9-4940-89f0-6961cf4b115e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
