{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9149d3e-e50f-40de-b705-65b5dc9bfc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a60954-56df-454f-84bc-79e1bbe111d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5368733d-087e-4db3-848c-64ece8e69746",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289de093-caf4-44b4-a927-0023d3fe5b24",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcf8ef80-f716-4d87-988e-170d75564479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(question, reply_input, reply_target):\n",
    "    \n",
    "    def subsequent_mask(size):\n",
    "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "        return mask.unsqueeze(0)\n",
    "    \n",
    "    question_mask = question!=0\n",
    "    question_mask = question_mask.to(device)\n",
    "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n",
    "     \n",
    "    reply_input_mask = reply_input!=0\n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n",
    "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n",
    "    reply_target_mask = reply_target!=0              # (batch_size, max_words)\n",
    "    \n",
    "    return question_mask, reply_input_mask, reply_target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040fc63-dda5-4f35-9adb-20decee3cb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e29f784-3304-4364-97e7-5b8e5ac2759f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ba54de-4646-4616-bf31-26be45acbb8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7af8b34-e8c0-4681-8ef4-46621b6750ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_emb, seq_length, emb_drop):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_emb)\n",
    "        self.sr_d_emb = np.sqrt(d_emb)\n",
    "        self.dropout = nn.Dropout(emb_drop)\n",
    "        self.pe = nn.Parameter(torch.rand(seq_length, d_emb))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = self.embed(x) * self.sr_d_emb\n",
    "        x = x + self.pe[: x.size(1)]\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c17d4f-8b6a-46cb-a769-92bb3e282045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c432fbda-1ce8-4de6-836c-0d1e65f27f31",
   "metadata": {
    "tags": []
   },
   "source": [
    "## transformer blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e99de9-c403-4aff-a1f9-b6bbca2ccf37",
   "metadata": {},
   "source": [
    "### Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba09a518-7be6-4cdf-9f69-5daf903f7619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Head_Attention(nn.Module):\n",
    "    def __init__(self, d_emb, d_hid, heads, decode = False):\n",
    "        super().__init__()\n",
    "        self.d_hid = d_hid\n",
    "        self.heads = heads\n",
    "        self.dim_per_head = self.d_hid // self.heads\n",
    "        \n",
    "        self.decode = decode\n",
    "        if self.decode:\n",
    "            self.q = nn.Linear(d_emb, self.d_hid, bias = False)\n",
    "            self.kv = nn.Linear(d_emb, self.d_hid * 2, bias = False)\n",
    "        else:\n",
    "            self.qkv = nn.Linear(d_emb, self.d_hid * 3, bias = False)\n",
    "        \n",
    "        self.unifyheads = nn.Linear(self.d_hid, d_emb)\n",
    "    \n",
    "    def self_attention(self, q, k, v, mask):\n",
    "        scores = torch.einsum('...ij,...kj->...ik', q, k) / np.sqrt(self.dim_per_head)\n",
    "        scores = scores.masked_fill(mask == 0, -float('inf'))\n",
    "        scores = F.softmax(scores, dim = -1)\n",
    "        return torch.einsum('...ij,...jk->...ik', scores, v)\n",
    "    \n",
    "    def forward(self, x, mask, y = None):\n",
    "        if self.decode:\n",
    "            q = self.q(y)\n",
    "            kv = self.kv(x)\n",
    "            k = kv[..., :self.d_hid]\n",
    "            v = kv[..., self.d_hid:]\n",
    "        else:\n",
    "            qkv = self.qkv(x)\n",
    "            q = qkv[..., :self.d_hid]\n",
    "            k = qkv[..., self.d_hid : self.d_hid * 2]\n",
    "            v = qkv[..., self.d_hid * 2 :]\n",
    "            \n",
    "        q = rearrange(q, '... i (h j) -> ... h i j', h = self.heads)\n",
    "        k = rearrange(k, '... i (h j) -> ... h i j', h = self.heads)\n",
    "        v = rearrange(v, '... i (h j) -> ... h i j', h = self.heads)\n",
    "                \n",
    "        scores = self.self_attention(q, k, v, mask)\n",
    "        scores = rearrange(scores, '... h i j -> ... i (h j)').contiguous()\n",
    "                      \n",
    "        return self.unifyheads(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f38d08-280e-42ec-820d-f1619c930ae1",
   "metadata": {},
   "source": [
    "### Gated Linear Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89e71539-a43f-4fdb-a395-1ef379b5fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.out_size = out_size\n",
    "        self.linear = nn.Linear(in_size, out_size * 2)\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        #x = x[..., : self.out_size] * x[..., self.out_size :].sigmoid()\n",
    "        x = torch.einsum('...i, ...i->...i', [x[..., : self.out_size], x[..., self.out_size :].sigmoid()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006fe90b-847a-45ca-91cd-40f227913a06",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd78f41d-5509-4bee-b765-9223fb612bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_layer(nn.Module):\n",
    "    def __init__(self, d_emb, d_hid, hidden_mult, heads, enc_drop):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(enc_drop)\n",
    "        \n",
    "        self.mha = Multi_Head_Attention(d_emb, d_hid, heads)\n",
    "        self.norm_1 = nn.LayerNorm(d_emb)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_emb, hidden_mult * d_emb),\n",
    "            #nn.PReLU(),\n",
    "            #nn.GELU(),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_mult * d_emb, d_emb)\n",
    "        )\n",
    "        #self.ff = GLU(d_emb, d_emb)\n",
    "\n",
    "        self.norm_2 = nn.LayerNorm(d_emb)\n",
    "        \n",
    "    def forward(self, x, q_mask):\n",
    "        attended = self.mha(x, q_mask)\n",
    "        x = attended + x\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm_1(x)\n",
    "        fed_for = self.ff(x)\n",
    "        x = fed_for + x\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45d1835-3c4f-49e0-a861-562bc113ce75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8147baa-095b-4e7e-90f0-48ab2cd1a9e3",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a08616c-84f3-4ff6-bbd3-14e6e5a40a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_layer(nn.Module):\n",
    "    def __init__(self, d_emb, d_hid, hidden_mult, heads, dec_drop):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dec_drop)\n",
    "        \n",
    "        self.mha_1 = Multi_Head_Attention(d_emb, d_hid, heads)\n",
    "        self.norm_1 = nn.LayerNorm(d_emb)\n",
    "        self.mha_2 = Multi_Head_Attention(d_emb, d_hid, heads, decode = True)\n",
    "        self.norm_2 = nn.LayerNorm(d_emb)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_emb, hidden_mult * d_emb),\n",
    "            #nn.PReLU(),\n",
    "            #nn.GELU(),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_mult * d_emb, d_emb)\n",
    "        )\n",
    "        #self.ff = GLU(d_emb, d_emb)\n",
    "    \n",
    "        self.norm_3 = nn.LayerNorm(d_emb)\n",
    "        \n",
    "    def forward(self, x, q_mask, y, r_mask):\n",
    "        attended = self.mha_1(y, r_mask)\n",
    "        y = attended + y\n",
    "        y = self.dropout(y)\n",
    "        y = self.norm_1(y)\n",
    "        attended = self.mha_2(x, q_mask, y)\n",
    "        y = attended + y\n",
    "        y = self.dropout(y)\n",
    "        y = self.norm_2(y)\n",
    "        fed_for = self.ff(y)\n",
    "        y = fed_for + y\n",
    "        y = self.dropout(y)\n",
    "        y = self.norm_3(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d64d96-26c5-48de-87b1-27900a6112a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0da790a2-332e-497a-b668-dc1fa4f638b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc3e4a07-4878-432f-8872-aef3d6eb04bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, model_hp):\n",
    "        super().__init__()\n",
    "        self.epochs = 0\n",
    "        self.losses = []\n",
    "        vocab_size_in, vocab_size_out, d_emb, hidden_mult, seq_length, order, dropouts, parallel = model_hp\n",
    "        emb_drop, enc_drop, dec_drop = dropouts\n",
    "        self.flag_parallel = parallel\n",
    "        self.deep = len(order)\n",
    "        \n",
    "        if vocab_size_in == vocab_size_out:\n",
    "            self.flag_voc_same = True\n",
    "        else:\n",
    "            self.flag_voc_same = False\n",
    "        \n",
    "        if self.flag_voc_same:\n",
    "            self.embedder = Embedder(vocab_size_in, d_emb, seq_length, emb_drop)\n",
    "        else:\n",
    "            self.embedder_enc = Embedder(vocab_size_in, d_emb, seq_length, emb_drop)\n",
    "            self.embedder_dec = Embedder(vocab_size_out, d_emb, seq_length, emb_drop)\n",
    "            \n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for d_hid, heads in order:\n",
    "            self.encoder.append(Encoder_layer(d_emb, d_hid, hidden_mult, heads, enc_drop))\n",
    "            self.decoder.append(Decoder_layer(d_emb, d_hid, hidden_mult, heads, dec_drop))\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(d_emb, d_emb * 2),\n",
    "            #nn.PReLU(),\n",
    "            #nn.GELU(),\n",
    "            nn.LeakyReLU(),\n",
    "            #GLU(d_emb, d_emb),\n",
    "            nn.Linear(d_emb * 2, vocab_size_out)\n",
    "            )\n",
    "        \n",
    "#         self.out = nn.DataParallel(self.out)\n",
    "    \n",
    "    def encode(self, x, q_mask):\n",
    "        if self.flag_voc_same:\n",
    "            x = self.embedder(x)\n",
    "        else:\n",
    "            x = self.embedder_enc(x)\n",
    "        for enc in self.encoder:\n",
    "            x = enc(x, q_mask)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x, q_mask, y, r_mask):\n",
    "        if self.flag_voc_same:\n",
    "            y = self.embedder(y)\n",
    "        else:\n",
    "            y = self.embedder_dec(y)\n",
    "        for dec in self.decoder:\n",
    "            y = dec(x, q_mask, y, r_mask)\n",
    "        return y\n",
    "    \n",
    "    def encode_decode(self, x, q_mask, y, r_mask):\n",
    "        if self.flag_voc_same:\n",
    "            x = self.embedder(x)\n",
    "            y = self.embedder(y)\n",
    "        else:\n",
    "            x = self.embedder_enc(x)\n",
    "            y = self.embedder_dec(y)\n",
    "        for i in range(self.deep):\n",
    "            x = self.encoder[i](x, q_mask)\n",
    "            y = self.decoder[i](x, q_mask, y, r_mask)\n",
    "        return y\n",
    "    \n",
    "    def forward(self, x, q_mask, y, r_mask):\n",
    "        if self.flag_parallel:\n",
    "            decoded = self.encode_decode(x, q_mask, y, r_mask)\n",
    "        else:\n",
    "            encoded = self.encode(x, q_mask)\n",
    "            decoded = self.decode(encoded, q_mask, y, r_mask)\n",
    "        out = self.out(decoded)\n",
    "        out = F.log_softmax(out, dim = -1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc5346-d1d4-4570-8304-d7626baa17a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c91927e-10a0-4d45-95a1-8201d81373de",
   "metadata": {},
   "source": [
    "## Optimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75725a0d-e956-4662-be6a-202cb1a528d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamWarmup:\n",
    "    def __init__(self, d_emb, warmup_steps, optimizer):\n",
    "        self.d_emb = d_emb\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.optimizer = optimizer\n",
    "        self.current_step = 0\n",
    "        self.lr = 0\n",
    "        \n",
    "    def get_lr(self):\n",
    "        return self.d_emb ** (-0.5) * min(self.current_step ** (-0.5), \n",
    "                                          self.current_step * self.warmup_steps ** (-1.5))\n",
    "        \n",
    "    def step(self):\n",
    "        # Increment the number of steps each time we call the step function\n",
    "        self.current_step += 1\n",
    "        lr = self.get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        # update the learning rate\n",
    "        self.lr = lr\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e6c63-ffee-462c-a4c1-b411ec09cb32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baecd14a-d366-451e-8659-f125e85a564a",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f1ecfee-a85c-461f-bbdd-a56ebdd42c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossWithLS(nn.Module):\n",
    "    def __init__(self, vocab_size_out, smooth):\n",
    "        super(LossWithLS, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction = 'none')\n",
    "        self.confidence = 1.0 - smooth\n",
    "        self.smooth = smooth\n",
    "        self.size = vocab_size_out\n",
    "        \n",
    "    def forward(self, prediction, target, mask):\n",
    "        \"\"\"\n",
    "        prediction of shape: (batch_size, max_words, vocab_size)\n",
    "        target and mask of shape: (batch_size, max_words)\n",
    "        \"\"\"\n",
    "        prediction = rearrange(prediction, 'i j k -> (i j) k')   # (batch_size * max_words, vocab_size)\n",
    "        target = rearrange(target, 'i j -> (i j)').contiguous()   # (batch_size * max_words)\n",
    "        mask = mask.view(-1).float()    # (batch_size * max_words)\n",
    "        labels = torch.full(prediction.shape, self.smooth / (self.size - 1)).to(device)\n",
    "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n",
    "        loss = (loss.sum(1) * mask).sum() / mask.sum()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a65c82-3ccd-4cfa-8fb6-7ee2d6bcab79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf07d0e3-fe98-45f1-b84d-1f81c2dc5743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea236b1-695f-4d68-be0f-3639bbf553a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4ee09b-b801-4e5c-849f-4bce06cb82f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
